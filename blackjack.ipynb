{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jing (Thomas) Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T04:14:19.204000Z",
     "start_time": "2019-08-01T04:14:19.198000Z"
    }
   },
   "outputs": [],
   "source": [
    "STATES = {-2, -1, 0, 1, 2}\n",
    "ACTIONS = {\"-1\", \"+1\"}\n",
    "GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T04:14:19.491000Z",
     "start_time": "2019-08-01T04:14:19.484000Z"
    }
   },
   "outputs": [],
   "source": [
    "def Actions(s):\n",
    "    if IsEnd(s):\n",
    "        return {}\n",
    "    return ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T04:14:20.020000Z",
     "start_time": "2019-08-01T04:14:20.013000Z"
    }
   },
   "outputs": [],
   "source": [
    "def IsEnd(s):\n",
    "    return s == 2 or s == -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:07:59.819000Z",
     "start_time": "2019-08-01T05:07:59.812000Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next(s):\n",
    "    return [s + 1, s - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're in state s and choose -1:\n",
    "- You have an 80% chance of reaching the state s−1.\n",
    "- You have a 20% chance of reaching the state s+1.\n",
    "\n",
    "If you're in state s and choose +1:\n",
    "- You have a 70% chance of reaching the state s+1.\n",
    "- You have a 30% chance of reaching the state s−1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:08:00.917000Z",
     "start_time": "2019-08-01T05:08:00.906000Z"
    }
   },
   "outputs": [],
   "source": [
    "def T(s, a, s_next):\n",
    "    if IsEnd(s):\n",
    "        return 0\n",
    "    if a == \"-1\":\n",
    "        if s_next == s - 1:\n",
    "            return 0.8\n",
    "        if s_next == s + 1:\n",
    "            return 0.2\n",
    "    if a == \"+1\":\n",
    "        if s_next == s + 1:\n",
    "            return 0.7\n",
    "        if s_next == s - 1:\n",
    "            return 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:08:01.950000Z",
     "start_time": "2019-08-01T05:08:01.941000Z"
    }
   },
   "outputs": [],
   "source": [
    "def Reward(s, a, s_next):\n",
    "    if s_next == -2:\n",
    "        return 20\n",
    "    if s_next == 2:\n",
    "        return 100\n",
    "    return -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q_{\\text{opt}}(s, a) = \n",
    "\\sum_{s'} T(s, a, s')[\\text{Reward(s, a, s')} + \\gamma V_{\\text{opt}}(s')] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:21.720000Z",
     "start_time": "2019-08-01T05:19:21.714000Z"
    }
   },
   "outputs": [],
   "source": [
    "def Q_opt(s, a, it):\n",
    "    return sum(T(s, a, s_next) * (Reward(s, a, s_next) + GAMMA * logs[it][s_next])\n",
    "              for s_next in get_next(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{\\text{opt}} =  \n",
    "\\begin{cases}\n",
    "0 & \\mbox{if } \\text{IsEnd}(s) \\\\\n",
    "\\text{max}_{a \\in \\text{Actions(s)}} Q_{\\text{opt}}(s, a) &\n",
    "\\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:22.286000Z",
     "start_time": "2019-08-01T05:19:22.279000Z"
    }
   },
   "outputs": [],
   "source": [
    "def V_opt(s, it):\n",
    "    if IsEnd(s):\n",
    "        return 0\n",
    "    return max(Q_opt(s, a, it) for a in Actions(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:23.855000Z",
     "start_time": "2019-08-01T05:19:23.850000Z"
    }
   },
   "outputs": [],
   "source": [
    "logs = {i: {s: 0 for s in STATES} for i in range(3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:24.798000Z",
     "start_time": "2019-08-01T05:19:24.788000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state -2:\t Vopt 0\n",
      "state -1:\t Vopt 0\n",
      "state 0:\t Vopt 0\n",
      "state 1:\t Vopt 0\n",
      "state 2:\t Vopt 0\n"
     ]
    }
   ],
   "source": [
    "for s, v in sorted(logs[0].items()):\n",
    "    print(\"state {}:\\t Vopt {}\".format(s, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:39.283000Z",
     "start_time": "2019-08-01T05:19:39.276000Z"
    }
   },
   "outputs": [],
   "source": [
    "for s in STATES:\n",
    "    logs[1][s] = V_opt(s, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:39.848000Z",
     "start_time": "2019-08-01T05:19:39.840000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state -2:\t Vopt 0\n",
      "state -1:\t Vopt 15.0\n",
      "state 0:\t Vopt -5.0\n",
      "state 1:\t Vopt 68.5\n",
      "state 2:\t Vopt 0\n"
     ]
    }
   ],
   "source": [
    "for s, v in sorted(logs[1].items()):\n",
    "    print(\"state {}:\\t Vopt {}\".format(s, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:41.329000Z",
     "start_time": "2019-08-01T05:19:41.322000Z"
    }
   },
   "outputs": [],
   "source": [
    "for s in STATES:\n",
    "    logs[2][s] = V_opt(s, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:42.385000Z",
     "start_time": "2019-08-01T05:19:42.375000Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state -2:\t Vopt 0\n",
      "state -1:\t Vopt 14.0\n",
      "state 0:\t Vopt 47.45\n",
      "state 1:\t Vopt 67.0\n",
      "state 2:\t Vopt 0\n"
     ]
    }
   ],
   "source": [
    "for s, v in sorted(logs[2].items()):\n",
    "    print(\"state {}:\\t Vopt {}\".format(s, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi_{\\text{opt}} = \\text{argmax}_{a \\in \\text{Actions}(s)} Q_{\\text{opt}}(s, a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:19:47.580000Z",
     "start_time": "2019-08-01T05:19:47.575000Z"
    }
   },
   "outputs": [],
   "source": [
    "def pi_opt(s, it):\n",
    "    return max([a for a in Actions(s)], key=lambda a: Q_opt(s, a, it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi_\\text{opt}(s)$ for iteration 2 (Calculated using $V_\\text{opt}$ in iteration 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T05:20:43.281000Z",
     "start_time": "2019-08-01T05:20:43.271000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal policy for state -1 is -1\n",
      "optimal policy for state 0 is +1\n",
      "optimal policy for state 1 is +1\n"
     ]
    }
   ],
   "source": [
    "for s in sorted(STATES):\n",
    "    if IsEnd(s):\n",
    "        continue\n",
    "    print(\"optimal policy for state {} is {}\".format(s, pi_opt(s, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the definition of acyclic, there is no cycle in the graph, which means once we visited a state, we will never go back to it. So, we can simply use dynamic programming to go through all the (s, a, s') triples in one pass. The value after that iteration will be set correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "Q_{\\text{opt}}'(s, a) \n",
    "&= \\sum_{s'} T(s, a, s')[\\text{Reward(s, a, s')} + \\gamma V_{\\text{opt}}(s')]\\\\\n",
    "&= \\sum_{s'} T(s, a, s')\\gamma[(\\frac{1}{\\gamma}) \\text{Reward(s, a, s')} + (1)V_{\\text{opt}}(s')]\\\\\n",
    "&= \\sum_{s'} \\gamma T(s, a, s')[\\frac{1}{\\gamma} \\text{Reward(s, a, s')} + (1)V_{\\text{opt}}(s')]\\\\\n",
    "&= \\sum_{s'} T'(s, a, s')[\\text{Reward'(s, a, s')} + \\gamma' V_{\\text{opt}}(s')]\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all $s'$:\n",
    "- $T'(s, a, s') = \\gamma T(s, a, s')$\n",
    "- $\\text{Reward'(s, a, s')} = \\frac{1}{\\gamma} \\text{Reward(s, a, s')}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above equation equals the old equation $Q_{\\text{opt}}$, the remaining term for the end state $o$ must equal to 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{split}\n",
    "T'(s, a, o)[\\text{Reward'(s, a, o)} + V_{\\text{opt}}(o)] \n",
    "&= T'(s, a, o)[\\text{Reward'(s, a, o)} + 0]\\\\\n",
    "&= T'(s, a, o)\\text{Reward'(s, a, o)}\\\\\n",
    "&= 0\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $T'(s, a, o)$ does not have to equal to 0, $\\text{Reward'(s, a, o)} = 0$\n",
    "\n",
    "$T'(s, a, o) = 1 - \\gamma$ (sum to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T06:54:42.698000Z",
     "start_time": "2019-08-01T06:54:42.692000Z"
    }
   },
   "source": [
    "### Question b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Console log of 4b helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T06:55:11.134000Z",
     "start_time": "2019-08-01T06:55:11.122000Z"
    }
   },
   "source": [
    "Small MDP\n",
    "- ValueIteration: 5 iterations\n",
    "- 27 total, 25 same, 2 different\n",
    "- 7.40740740741% diff\n",
    "\n",
    "Large MDP\n",
    "- ValueIteration: 15 iterations\n",
    "- 2745 total, 1574 same, 1171 different\n",
    "- 42.6593806922% diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Learning performed much better on small MDP than large MDP. This is probably due to the fact that it was unable to learn well in a much larger state space, and the identity feature extractor can't describe unexplored states (can't generalize). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ValueIteration: 15 iterations\n",
    "- 2745 total, 2040 same, 705 different\n",
    "- 25.6830601093% diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T06:54:42.698000Z",
     "start_time": "2019-08-01T06:54:42.692000Z"
    }
   },
   "source": [
    "### Question d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T10:55:32.613000Z",
     "start_time": "2019-08-01T10:55:32.599000Z"
    }
   },
   "source": [
    "- VI then Fixed: 8.47406666667\n",
    "- Double Q-learning: 9.14346666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning has higher expected rewards then FixedRL because it can adapt to the MDP with the new threshold. For the VI then FixedRL, its policies may be optimal for the original MDP but may not be optimal for the new threshold, and it can't adapt to the new one as it lacks the ability to learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
